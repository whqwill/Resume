Computing Distributed Representations for Polysemous Words

[Abstract]
Recently, machine learning especially deeplearning is very popular. Word vector is very im- portant tool for many natural language processing when using machine larning algorithms. There are many methods ([Bengio et al., 2003],[Collobert and Weston, 2008] and [Mikolov et al., 2013]) to generate word vector, usually we call this process word embedding, and call such word vector distributed representation. Most of word embedding methods can not generate word vector based on wordâ€™s context, that is similar words have similar vectors. But there are still problems when doing some tasks like decting word senses. Some pol- ysemous words can represent different mearnings in different contexts. Acoddingly, each polysemous word should have several vector. Some models have been successively proposed ([Huang et al., 2012],[Tian et al., 2014] and [Neelakantan et al., 2015a]) to do sense embed- dings to represent word senses. Our thesis investigates and improves current methods with multiple senses per word . Specifically we extend the bacis word embedding model, i.e. word2vec ([Mikolov et al., 2013]), to build a sense assignmen model. In short, each word can have several senses in each word, we use some score function to decide the best sense for each word, through a lot of unsupersived learning, our model adjust senses for each word in sentences and finally generate sense vectors. This thesis implements this model in Spark to be able to execute in parallel and trains sense vectors with Wikipedia corpus. We evaluate sense vectors by doing word similarity tasks using SCWS (Contextual word Similarityes) dataset from [Huang et al., 2012] and WordSim-353 dataset from [Finkelstein et al., 2001] .
